深度学习

深度神经网络（Deep Neural Networks）

### 感知机

**感知机**（Perceptron），可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。

perceptron有四个部分：输入输出；权重和偏置；Net sum；激活函数

```
1.单层感知机(Perceptron)只能用于简单的线性分类问题，对非线性问题束手无策，例如无法区分XOR分类问题。但是，它的计算速度非常快。
2.多层感知机(Multilayer Perceptron)是一种深度学习模型。它使用一个或两个隐藏层。主要的优点是它们可以用于复杂的问题。然而，他们通常需要长时间来训练。此外，过大的深度易导致梯度消失，bias被放大等问题。
```

感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法

### 梯度消失和梯度爆炸
- 梯度消失
一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid
- 梯度爆炸
梯度爆炸一般出现在深层网络和权值初始化值太大的情况下

当对激活函数进行求导时，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失

### 反向传播 
目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。
深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数 ，因此整个深度网络可以视为是一个复合的非线性多元函数。
从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足，另外多说一句，Hinton提出capsule的原因就是为了彻底抛弃反向传播，如果真能大范围普及，那真是一个革命
反向传播算法在生物学上很难成立，很难相信神经系统能够自动形成与正向传播对应的反向传播结构（这需要精准地求导数，对矩阵转置，利用链式法则，并且解剖学上从来也没有发现这样的系统存在的证据）。反向传播算法更像是仅仅为了训练多层 NN 而发展的算法。其次，反向传播算法需要 SGD 等方式进行优化，这是个高度非凸的问题，其数学性质是堪忧的，而且依赖精细调参
反向传播是相对于目标函数计算的，没有目标函数就无法进行反向传播。如果你无法评估预测值和标签（实际或训练数据）的 value 值，你就没有目标函数。因此，为了实现“无监督学习”，就需要抛弃计算梯度的能力。

### 卷积神经网路（ Convolutional Neural Network，CNN ）
是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网路由一个或多个卷积层和顶端的全连通层（对应经典的神经网路）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网路能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网路在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。

INPUT（输入层）-CONV（卷积层）-RELU（激活函数）-POOL（池化层）-FC（全连接层）
全连接层（全连接层和常规神经网络中的一样）

**全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，从而导致参数数量膨胀。**

#### 卷积 Convolutional
卷积操作的用处了：通过滤波器的覆盖和对应运算来降维，实现用输出图像中更亮的像素表示原始图像中存在的边缘。通常，卷积有助于我们找到特定的局部图像特征（如边缘）

#### 池化 Pooling layer
池化过程在一般卷积过程后。
图像中的相邻像素倾向于具有相似的值，因此通常卷积层相邻的输出像素也具有相似的值。这意味着，卷积层输出中包含的大部分信息都是冗余的。
池化（pooling） 的本质，其实就是采样。Pooling 对于输入的 Feature Map，选择某种方式，一般通过简单的最大值、最小值或平均值操作完成，对其进行降维压缩，以加快运算速度。
池化的作用：
1.保留主要特征的同时减少参数和计算量，防止过拟合。
2.invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

#### 池化过程的疑问（source:Hinton）
1.池化过程并没有很好地模仿大脑中形状知觉的心理过程：它不能解释为什么人类能将内在的坐标系映射到物体上，以及为什么这些坐标系这么重要；
2.池化解决的问题是错误的：我们想要的是信息的同变性而非不变性，是理清信息而非丢弃信息；
3.池化没有利用底层线性结构：它没有利用在图形中能很好地处理方差最大来源的自然线形流形；
4.池化在处理动态路由时很差劲：我们需要将输入信息的每一部分路由到知道如何处理它的神经元中，找到最佳的路径就是解析图像

### Batch Normalization

Internal Covariate Shift：在深层网络训练的过程中，由于网络中参数变化，带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重

Batch Normalization，即通过固定每一层网络输入值的分布来对减缓Internal Covariate Shift问题

####  Batch Normalization的优势

Batch Normalization在实际工程中被证明了能够缓解神经网络难以训练的问题，BN具有的有事可以总结为以下三点：

**（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度**

BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。

**（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定**

**（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**

**（4）BN引入了随机噪声，具有一定的正则化效果**

